{"cells":[{"cell_type":"markdown","metadata":{"id":"EqhX6FbHCkYz"},"source":["**Shibu Mohapatra MSC AI**"]},{"cell_type":"markdown","metadata":{"id":"QCRey7WsCBbD"},"source":["# UNIT 1: Introduction to Natural Language Processing"]},{"cell_type":"markdown","metadata":{"id":"6174IC0tz9cx"},"source":["## Text processing terminology"]},{"cell_type":"markdown","metadata":{"id":"C-_vv3mzK6H4"},"source":["### Noise removal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":387,"status":"ok","timestamp":1648308268404,"user":{"displayName":"Shibu Mohapatra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ9xZwtQZL9vw1UHNOP-mEy8i6a5ym0bypMHvWpA=s64","userId":"07626953913233812311"},"user_tz":-330},"id":"bNyUEaD4JZUv","outputId":"672511d0-4b47-4c66-80c1-dc196e34e16d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Five fantastic fish flew off to find faraway functions Maybe find another five fantastic fish Find my fish with a function please\n"]}],"source":["import re\n"," \n","text = \"Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish? Find my fish with a function please!\"\n","# remove punctuation\n","result = re.sub(r'[\\.\\?\\!\\,\\:\\;\\\"]', '', text)\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"P-GrBUOWMOYm"},"source":["### Text Normalizing - Stemming"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1514,"status":"ok","timestamp":1648308271521,"user":{"displayName":"Shibu Mohapatra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ9xZwtQZL9vw1UHNOP-mEy8i6a5ym0bypMHvWpA=s64","userId":"07626953913233812311"},"user_tz":-330},"id":"FjqqgkbtMSbl","outputId":"07c95361-3daf-4533-c0ee-fb7109b46807"},"outputs":[{"name":"stdout","output_type":"stream","text":["['So', 'mani', 'squid', 'are', 'jump']\n"]}],"source":["from nltk.stem import PorterStemmer\n"," \n","tokenized = [\"So\", \"many\", \"squids\", \"are\", \"jumping\"]\n"," \n","stemmer = PorterStemmer()\n","stemmed = [stemmer.stem(token) for token in tokenized]\n"," \n","print(stemmed)"]},{"cell_type":"markdown","metadata":{"id":"8QoCDKj6MoRW"},"source":["### Lemmatization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1123,"status":"ok","timestamp":1648308273277,"user":{"displayName":"Shibu Mohapatra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ9xZwtQZL9vw1UHNOP-mEy8i6a5ym0bypMHvWpA=s64","userId":"07626953913233812311"},"user_tz":-330},"id":"eG00hQzOMtrS","outputId":"fa758d2c-6a8d-47b9-fd4f-dd8cc553b1a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2008,"status":"ok","timestamp":1648308276234,"user":{"displayName":"Shibu Mohapatra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ9xZwtQZL9vw1UHNOP-mEy8i6a5ym0bypMHvWpA=s64","userId":"07626953913233812311"},"user_tz":-330},"id":"zy-YSOEbNCOH","outputId":"2cba2a7e-8af5-4a6b-d243-c677d3c2c146"},"outputs":[{"name":"stdout","output_type":"stream","text":["['squid', 'jump']\n"]}],"source":["from nltk.stem import WordNetLemmatizer\n"," \n","tokenized = [\"squids\",\"jumps\"]\n"," \n","lemmatizer = WordNetLemmatizer()\n","lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n"," \n","print(lemmatized)"]},{"cell_type":"markdown","metadata":{"id":"vIxOA67gPcX7"},"source":["### Stopword"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":643,"status":"ok","timestamp":1648308330814,"user":{"displayName":"Shibu Mohapatra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ9xZwtQZL9vw1UHNOP-mEy8i6a5ym0bypMHvWpA=s64","userId":"07626953913233812311"},"user_tz":-330},"id":"VEz9HXOnPhMP","outputId":"a14d6690-3e6a-425f-90dd-70d613a950ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1648308333210,"user":{"displayName":"Shibu Mohapatra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ9xZwtQZL9vw1UHNOP-mEy8i6a5ym0bypMHvWpA=s64","userId":"07626953913233812311"},"user_tz":-330},"id":"O1NHa8MlPaZZ","outputId":"5585b914-3cba-4e32-c3d3-16b802f9b621"},"outputs":[{"name":"stdout","output_type":"stream","text":["['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n","['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"]}],"source":["from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n"," \n","example = \"This is a sample sentence, showing off the stop words filtration.\"\n","stop_words = set(stopwords.words('english')) \n","\n","word_tokens = word_tokenize(example)\n"," \n","# remove stopwords from tokens in dataset\n","statement_no_stop = [word for word in example if word not in stop_words]\n","statement_no_stop = []\n","\n","for w in word_tokens:\n","  if w not in stop_words:\n","    statement_no_stop.append(w)\n","\n","print(word_tokens)\n","print(statement_no_stop)"]},{"cell_type":"markdown","metadata":{"id":"gC4TrkrHVgA1"},"source":["### TF-IDF Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ryhSzGaUUhpx"},"outputs":[],"source":["paragraph = \"Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish? Find my fish with a function please!\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1rCQOV5UrZD"},"outputs":[],"source":["#these steps are the same as above\n","\n","sentences = nltk.sent_tokenize(paragraph)\n","\n","corpus = []\n","\n","#These steps can be considered as light text pre-processing\n","for i in range(len(sentences)):\n","  review = re.sub('[^a-zA-Z]', ' ', sentences[i]) #replacing characters with space other than alphabeticals \n","  review = review.lower() #lower-casing the words\n","  review = review.split() #removind the spaces before and after words\n","  review = [stemmer.stem(word) for word in review if word not in set(stopwords.words('english'))] #removing stop-words\n","  review = ' '.join(review) #joining each words separated by a space in between\n","  corpus.append(review) #add above item to a new list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTR6FYmzVGPw"},"outputs":[],"source":["#creating TF-IDF Model\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","tf_idf = TfidfVectorizer()\n","tf_idf_vector = tf_idf.fit_transform(corpus).toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVi2vWjtVWrM"},"outputs":[],"source":["from gensim.models import Word2Vec\n","\n","#These steps can be considered as light text pre-processing\n","for i in range(len(sentences)):\n","  review = re.sub('[^a-zA-Z]', ' ', sentences[i]) #replacing characters with space other than alphabetical ones\n","  review = review.lower() #lower-casing the words\n","  review = review.split() #removind the spaces before and after words\n","  review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words('english'))] #removing stop-words\n","  review = ' '.join(review) #joining each words separated by a space in between\n","  corpus.append(review) #add above item to a new list\n","corpus = [nltk.word_tokenize(word) for word in corpus] #word tokenizing the sentences in the \"corpus\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1648308345287,"user":{"displayName":"Shibu Mohapatra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ9xZwtQZL9vw1UHNOP-mEy8i6a5ym0bypMHvWpA=s64","userId":"07626953913233812311"},"user_tz":-330},"id":"RVL8EHx9VayF","outputId":"c68e4149-28fe-42b2-b41e-22f638441fdf"},"outputs":[{"data":{"text/plain":["[['five', 'fantast', 'fish', 'flew', 'find', 'faraway', 'function'],\n"," ['mayb', 'find', 'anoth', 'five', 'fantast', 'fish'],\n"," ['find', 'fish', 'function', 'pleas'],\n"," ['five', 'fantastic', 'fish', 'flew', 'find', 'faraway', 'function'],\n"," ['maybe', 'find', 'another', 'five', 'fantastic', 'fish'],\n"," ['find', 'fish', 'function', 'please']]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["corpus"]},{"cell_type":"markdown","metadata":{"id":"hweT5D3wLO1a"},"source":["## Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1755,"status":"ok","timestamp":1648472790919,"user":{"displayName":"Shibu Mohapatra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ9xZwtQZL9vw1UHNOP-mEy8i6a5ym0bypMHvWpA=s64","userId":"07626953913233812311"},"user_tz":-330},"id":"LCN2uDsQLQbZ","outputId":"11dfe3e3-8b17-447b-deef-ff78e039758f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1648472790920,"user":{"displayName":"Shibu Mohapatra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ9xZwtQZL9vw1UHNOP-mEy8i6a5ym0bypMHvWpA=s64","userId":"07626953913233812311"},"user_tz":-330},"id":"b57SFozcLgeh","outputId":"cdcf1ddb-9435-4715-be74-4e21fb950a26"},"outputs":[{"name":"stdout","output_type":"stream","text":["['This', 'is', 'a', 'text', 'to', 'tokenize', '.', 'This', 'is', 'also', 'a', 'sentence', '.']\n","['This is a text to tokenize.', 'This is also a sentence.']\n"]}],"source":["from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n"," \n","text1 = \"This is a text to tokenize. This is also a sentence.\"\n","tokenized = word_tokenize(text1)\n","sentence = sent_tokenize(text1) \n","\n","print(tokenized)\n","print(sentence)"]},{"cell_type":"markdown","metadata":{"id":"KAh-63q91e8m"},"source":["## Sentence Segmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nu3dn6IW1l6T"},"outputs":[],"source":["#import spacy library\n","import spacy\n","  \n","#load core english library\n","nlp = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":382,"status":"ok","timestamp":1648308553332,"user":{"displayName":"Shibu Mohapatra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ9xZwtQZL9vw1UHNOP-mEy8i6a5ym0bypMHvWpA=s64","userId":"07626953913233812311"},"user_tz":-330},"id":"hikEL3Cx1vGO","outputId":"ce95b57d-856a-4c2d-b087-f4d1a8cafeb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["I like Artificial Intelligence.\n","I taught myself many concepts of Natural Language Processing.\n","I have a vision to become a Data Scientist.\n"]}],"source":["#take unicode string  \n","#here u stands for unicode\n","\n","doc = nlp(\"I like Artificial Intelligence. I taught myself many concepts of Natural Language Processing. I have a vision to become a Data Scientist.\")\n","\n","#to print sentences\n","for sent in doc.sents:\n","  print(sent)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1648308555953,"user":{"displayName":"Shibu Mohapatra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ9xZwtQZL9vw1UHNOP-mEy8i6a5ym0bypMHvWpA=s64","userId":"07626953913233812311"},"user_tz":-330},"id":"bAVaBiTe13rf","outputId":"8e60740e-fa74-4b4b-8299-a68dc8e48318"},"outputs":[{"data":{"text/plain":["I taught myself many concepts of Natural Language Processing."]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["#converting the generator object result in to list\n","doc1 = list(doc.sents)\n","  \n","#Now we can use it randomly as\n","doc1[1]"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMiJSaSqCI9GVgAEtqe2fD7","collapsed_sections":["6174IC0tz9cx","hweT5D3wLO1a","KAh-63q91e8m"],"name":"UNIT 1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
