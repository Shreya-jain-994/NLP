{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2_normalize.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
        "input_str = input_str.lower()\n",
        "print(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql7StzFXg6BS",
        "outputId": "904e1215-ff31-4ce9-948a-c3f6270d6b83"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "input_str = 'Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.'\n",
        "result = re.sub(r'\\d+', '', input_str)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD68CGT_j2-5",
        "outputId": "df10cda0-178f-4739-98dc-38db5f75d5f0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "regular_punct = list(string.punctuation)\n",
        "def remove_punctuation(text,punct_list):\n",
        "    for punc in punct_list:\n",
        "        if punc in text:\n",
        "            text = text.replace(punc, ' ')\n",
        "    return text.strip()\n",
        "\n",
        "remove_punctuation(\"Good Morning! How are you?\", regular_punct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bn962aAskdLe",
        "outputId": "dfc8cb4c-9a20-4a85-ce7c-7117157fd117"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Good Morning  How are you'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = \"\\t a string example\\t\"\n",
        "input_str = input_str.strip()\n",
        "input_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GNrULqObmON1",
        "outputId": "d7ed756d-9a34-46e6-926e-769b35f3a8a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a string example'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expanding abbreviation and Canonicalization"
      ],
      "metadata": {
        "id": "u0_DyCUfqCdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd464fa1-2403-42ce-90bf-d71a8039e283",
        "id": "OqbRwuXVnh8Q"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"This is the practical implementation of Natural Language Processing.\"\n",
        "\n",
        "print(word_tokenize(text1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99b1136c-56a8-4607-9dfc-ce2627ba083d",
        "id": "2kaXmHpAnh8T"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'the', 'practical', 'implementation', 'of', 'Natural', 'Language', 'Processing', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(text1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d4f759-07fe-408a-a261-7449fab8456d",
        "id": "poWsyiW4nh8U"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is the practical implementation of Natural Language Processing.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokens = word_tokenize(input_str)\n",
        "result = [i for i in tokens if not i in stop_words]\n",
        "print (result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNIF_bm_nT9b",
        "outputId": "89ed1dea-3dd0-43db-e639-8d9033b50003"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer= PorterStemmer()\n",
        "input_str= \"There are several types of stemming algorithms.\"\n",
        "\n",
        "input_str=word_tokenize(input_str)\n",
        "\n",
        "for word in input_str:\n",
        "    print(stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeMdNDNVoAm_",
        "outputId": "98e333f2-bfac-4f5d-d897-fb205bc38cb1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there\n",
            "are\n",
            "sever\n",
            "type\n",
            "of\n",
            "stem\n",
            "algorithm\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb0wFjdGoIHH",
        "outputId": "ab623438-b8e0-4c38-e2f0-872231711905"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()\n",
        "input_str= \"been had done languages cities mice\"\n",
        "\n",
        "input_str=word_tokenize(input_str)\n",
        "\n",
        "for word in input_str:\n",
        "    print(lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeEIVcYCoR2f",
        "outputId": "62378012-c191-415a-c7dc-ea886ffefbdf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "been\n",
            "had\n",
            "done\n",
            "language\n",
            "city\n",
            "mouse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXEuW1CVoVYy",
        "outputId": "47cd0fde-2626-428d-85b5-d56c645aa901"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_str= \"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n",
        "result = TextBlob(input_str)\n",
        "print(result.tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty-Z8UmkoiDv",
        "outputId": "91d1f114-75a6-41a3-c78d-0edaa673e3e1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
          ]
        }
      ]
    }
  ]
}