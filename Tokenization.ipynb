{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P1_tokenize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical No: 1**"
      ],
      "metadata": {
        "id": "7jyj3wR1ghVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UuYf_bQLLG0",
        "outputId": "085295d2-3d0b-4402-fda4-b24757ed822d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"This is the first practical implementation of Natural Language Processing.\"\n",
        "\n",
        "print(word_tokenize(text1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JamnhU-5L5eh",
        "outputId": "3f8a022b-7241-462d-fab4-f83f299d7eb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'the', 'first', 'practical', 'implementation', 'of', 'Natural', 'Language', 'Processing', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(text1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC1pML2bOEgZ",
        "outputId": "f9e3cbb1-85ab-400e-a729-a807b379bb19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is the first practical implementation of Natural Language Processing.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2=\"HELLO EVERYONE\"\n",
        "text2=re.sub(r\"[^a-zA-Z0-9]\", \" \", text2.lower())\n",
        "print(text2)\n",
        "words=text2.split()\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_TJlJIUOfuc",
        "outputId": "55b154d4-8c92-4a7a-985f-26b4df6e68dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello everyone\n",
            "['hello', 'everyone']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3=\"Guys come to Dubai. I am happy.happy, happy$, .*happy\"\n",
        "x=re.search(\"^Guys.*happy$\", text3)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpOsaklOR1RD",
        "outputId": "3ce076b4-d554-4400-d0b5-4dc2f45cd215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 53), match='Guys come to Dubai. I am happy.happy, happy$, .*h>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkeuS2grUXZt",
        "outputId": "62e7926f-a35a-4fcf-dcdb-0e9900a58679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgTt1XC1fyoC"
      },
      "outputs": [],
      "source": [
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "word_tokens = word_tokenize(example_sent)\n",
        " \n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "  if w not in stop_words:\n",
        "    filtered_sentence.append(w)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCn5j2Vsi3KO",
        "outputId": "416d37bb-267f-4e36-9d88-0a95d1995cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ]
    }
  ]
}